{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  加载外部数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京是中华人民共和国的首都。\n",
      "count(北京是哪个国家的首都) =10 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 将数据切分成\n",
    "from dataclasses import dataclass, field\n",
    "from math import ceil, floor\n",
    "from enum import Enum\n",
    "from typing import List, Literal, TypedDict, Optional\n",
    "import tiktoken\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class MessageRole(Enum):\n",
    "    # chat role\n",
    "    User = \"user\"\n",
    "    System = \"system\"\n",
    "    Asisstant = \"asisstant\"\n",
    "    # inner role\n",
    "    You = \"you\"\n",
    "    YourComputer = \"your computer\"\n",
    "class MessageType(Enum):\n",
    "    AIResponse = \"ai_response\"\n",
    "    ActionResults = \"action_results\"\n",
    "\n",
    "\n",
    "# MessageRole = Literal[\"system\", \"user\", \"assistant\"]\n",
    "# MessageType = Literal[\"ai_response\", \"action_results\"]\n",
    "\n",
    "\n",
    "class MessageDict(TypedDict):\n",
    "    role: MessageRole\n",
    "    content: str\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: MessageRole\n",
    "    content: str\n",
    "    # TODO msg_type 的意义是什么？\n",
    "    msg_type: Optional[MessageType] = None\n",
    "\n",
    "    def raw(self) -> MessageDict:\n",
    "        return {\"role\": self.role, \"content\": self.content}\n",
    "\n",
    "\n",
    "def count_message_tokens(messages: List[Message], model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "    if model.startswith(\"gpt-3.5\"):\n",
    "        tokens_per_message = 4\n",
    "        tokens_per_name = -1\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.raw().items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|> assistant<|message>\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def count_string_tokens(string: str, model_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    return len(encoding.encode(string))  \n",
    "\n",
    "\n",
    "def get_completion_from_messages(messages, model='gpt-3.5-turbo', temperature=0, max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def create_msg(msg:str):\n",
    "    \n",
    "    messages = [\n",
    "   \n",
    "        {\"role\": \"user\", \"content\": msg},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "load_dotenv(\"../etc/dev/.env\")\n",
    "openai_api_key: str = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "msg = \"北京是哪个国家的首都\"\n",
    "print(get_completion_from_messages(messages=create_msg(msg)))\n",
    "\n",
    "print(f'count({msg}) ={count_string_tokens(msg)} ')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1972, which is longer than the specified 1763\n",
      "Created a chunk of size 2452, which is longer than the specified 1763\n",
      "Created a chunk of size 4954, which is longer than the specified 1763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process done 1 / 15\n",
      "process done 2 / 15\n",
      "process done 3 / 15\n",
      "process done 4 / 15\n",
      "process done 5 / 15\n",
      "process done 6 / 15\n",
      "process done 7 / 15\n",
      "process done 8 / 15\n",
      "process done 9 / 15\n",
      "process done 10 / 15\n",
      "process done 11 / 15\n",
      "process done 12 / 15\n",
      "process done 13 / 15\n",
      "process done 14 / 15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import traceback\n",
    "template_name = \"./kg/kg_industry_ana.txt\"\n",
    "template_str = open(template_name).read()\n",
    "import jinja2\n",
    "environment = jinja2.Environment()\n",
    "template = environment.from_string(template_str)\n",
    "\n",
    "content = open('data.txt').read()\n",
    "# print(content) \n",
    "template_count = count_string_tokens(template_str)\n",
    "OPENAI_MAX_TOKEN = 4096 - 300\n",
    "content_token_count = OPENAI_MAX_TOKEN - template_count - 700\n",
    "# print(f\"template count is {template_count}, conent_count is {content_token_count}\")\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_spliter = CharacterTextSplitter(\n",
    "        separator=\"\\n\", chunk_size=content_token_count, chunk_overlap=100, length_function=len\n",
    "    )\n",
    "# print(para)\n",
    "tmp_para_list = text_spliter.split_text(content)\n",
    "# print(len(tmp_para_list))\n",
    "tmp_title_list = []\n",
    "tmp_meta_list = []\n",
    "index = 0\n",
    "num_para = len(tmp_para_list)\n",
    "max_count = -1\n",
    "max_item = ''\n",
    "output_file = 'output/600271.kg_industry_ana_in_{}.txt'\n",
    "openai_out = 'output/600271.kg_industry_ana.openaiout.jsonl'\n",
    "openai_raw_out = 'output/600271.kg_industry_ana.openaiout.raw'\n",
    "writer = open(openai_out, \"a\")\n",
    "raw_writer = open(openai_raw_out, \"w\")\n",
    "\n",
    "for para_item in tmp_para_list:\n",
    "    item = template.render(content_fragment=para_item)\n",
    " \n",
    "    \n",
    "    \n",
    "    token_count = count_string_tokens(item)\n",
    "    if token_count > 0 and token_count < OPENAI_MAX_TOKEN:\n",
    "        open(output_file.format(index), \"w\").write(item)\n",
    "        \n",
    "        data = get_completion_from_messages(create_msg(item))\n",
    "        try:\n",
    "            raw_writer.write(f\"{index}\\n\\n\" + data + \"\\n\\n\\n\")\n",
    "            data_str = json.loads(data)\n",
    "            writer.write(json.dumps(data_str, ensure_ascii=False) + \"\\n\")\n",
    "            print(f\"process done {index+1} / {num_para}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            print(f\"process wrong {index+1} / {num_para}\")\n",
    "            \n",
    "        time.sleep(1)\n",
    "        index += 1 \n",
    "    if token_count > max_count:\n",
    "        max_count = token_count\n",
    "        max_item = item\n",
    "        \n",
    "# print(f\"MAX TOKEN COUNT IS {max_count}\")\n",
    "# print(max_item)\n",
    "print(len(tmp_para_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2886\n",
      "[\n",
      "    {\"公司所处的行业是什么,公司从事哪些主要的什么业务\": \"航天信息公司从事信息技术业务，主要涉及数字政府和企业数字化两个产业\"},\n",
      "    {\"行业基本情况、周期性特点\": \"信息技术行业处于成熟阶段，市场规模较大，竞争激烈，增长趋缓\"},\n",
      "    {\"公司所处的行业地位怎样？\": \"航天信息公司在信息技术行业中具有一定的市场份额，是国内领先的信息技术集团之一\"},\n",
      "    {\"最近是否有新的和公司所处行业相关的的法律、行政法规、部门规章、行业政策？这些内容对行业有什么影响\": \"最近发布了《关于加强数字政府建设的指导意见》和《关于构建数据基础制度更好发挥数据要素作用的意见》，对行业发展有积极影响\"},\n",
      "    {\"管理层对行业格局和趋势的分析和展望是什么，公司在这种情况下的优势和劣势是什么\": \"管理层认为数字经济、数字化产业发展趋势明显，公司在数字政府、企业数字化领域具备领先地位和一定优势基础，但需要保持竞争优势和适应变化的能力\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "## merge\n",
    "lines = open(openai_out, 'r').readlines()\n",
    "data = []\n",
    "for line in lines:\n",
    "    item_info_list = json.loads(line.strip())\n",
    "    result = {}\n",
    "    for item in item_info_list:\n",
    "        for key, value in item.items():\n",
    "            if value.find(\"未提及\") != -1 or value.find(\"不适用\") != -1:\n",
    "                continue\n",
    "            else:\n",
    "                result[key] = value\n",
    "                # print(key, value)\n",
    "    if len(result) > 0:\n",
    "        data.append(result)\n",
    "openai_merge_out = 'output/600271.kg_industry_ana.openaiout.merge.jsonl'\n",
    "openai_merge_raw_out = 'output/600271.kg_industry_ana.openaiout.merge.raw'\n",
    "writer = open(openai_merge_out, 'w')\n",
    "raw_writer = open(openai_merge_raw_out, \"w\")\n",
    "template_name = \"./kg/kg_industry_ana_merge.txt\"\n",
    "template_str = open(template_name).read()\n",
    "import jinja2\n",
    "environment = jinja2.Environment()\n",
    "template = environment.from_string(template_str)\n",
    "prompts = template.render(enumerate=enumerate, lines=data)\n",
    "# print(prompts)\n",
    "open(\"/tmp/tmp.tmp.txt\", 'w').write(prompts)\n",
    "print(count_string_tokens(prompts))\n",
    "\n",
    "data = get_completion_from_messages(create_msg(prompts), model='gpt-3.5-turbo-16k-0613')\n",
    "try:\n",
    "    print(data)\n",
    "    raw_writer.write(f\"{index}\\n\\n\" + data + \"\\n\\n\\n\")\n",
    "    data_str = json.loads(data)\n",
    "    writer.write(json.dumps(data_str, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            \n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print(f\"process wrong {index+1} / {num_para}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
