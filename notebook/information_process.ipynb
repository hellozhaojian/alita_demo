{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 管理层信息抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /Users/alchemy_taotaox/Desktop/next/my_test/* ../corpus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法基本思路\n",
    "1. 扫描pdf 获得页眉页脚\n",
    "2. 扫描pdf 获得最小和最大的x坐标\n",
    "    如果一个字符是最小的坐标，那么就和之前的行合并为一个段落\n",
    "    如果一个字符是最大的坐标，那么就和之前的\n",
    "2. 提取文档中的表格和字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfplumber.pdf import PDF\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def is_header(str, header):\n",
    "    if header is None:\n",
    "        return False\n",
    "    return str == header\n",
    "\n",
    "def is_foot(str, foot):\n",
    "    if foot is None:\n",
    "        return False\n",
    "    \n",
    "    s = re.sub(r'\\d+', '#num#', str)\n",
    "    return s == foot\n",
    "\n",
    "def get_foot_header(pdf:PDF, check_pages_num=20, threshold = 0.9):\n",
    "    head_counter = Counter()\n",
    "    foot_counter = Counter()\n",
    "    pages_num = len(pdf.pages)\n",
    "    if pages_num < check_pages_num:\n",
    "        check_pages_num = pages_num\n",
    "    for i in range(check_pages_num):\n",
    "        text_lines = pdf.pages[i].extract_words()\n",
    "        top_line = text_lines[0]['text']\n",
    "        bottom_line = text_lines[-1]['text']\n",
    "        bottom_line = re.sub(r'\\d+', '#num#', bottom_line)\n",
    "        head_counter.update([top_line])\n",
    "        foot_counter.update([bottom_line])\n",
    "        \n",
    "    most_common_head_word, count_head = head_counter.most_common(1)[0]\n",
    "    most_common_foot_word, count_foot = foot_counter.most_common(1)[0]\n",
    "    head, foot = None, None\n",
    "    if count_head *1./check_pages_num >= threshold:\n",
    "        head = most_common_head_word\n",
    "    if count_foot *1./check_pages_num >= threshold:\n",
    "        foot = most_common_foot_word\n",
    "    return head, foot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def check_bboxes(word, table_bbox):\n",
    "    \"\"\"\n",
    "    Check whether word is inside a table bbox.\n",
    "    \"\"\"\n",
    "    l = word['x0'], word['top'], word['x1'], word['bottom']\n",
    "    r = table_bbox\n",
    "    return l[0] > r[0] and l[1] > r[1] and l[2] < r[2] and l[3] < r[3]\n",
    "\n",
    "\n",
    "\n",
    "def get_lines_from_page(page, head, foot):\n",
    "    tables = page.find_tables()\n",
    "    table_bboxes = [i.bbox for i in tables]\n",
    "    tables = [{'table': i.extract(), 'top': i.bbox[1]} for i in tables]\n",
    "    non_table_words = [word for word in page.extract_words() if not any(\n",
    "        [check_bboxes(word, table_bbox) for table_bbox in table_bboxes])]\n",
    "    lines = []\n",
    "    lines_x0_x1 =[]\n",
    "    for cluster in pdfplumber.utils.cluster_objects(\n",
    "        non_table_words + tables, itemgetter('top'), tolerance=5):\n",
    "        if 'text' in cluster[0]:\n",
    "            x_0 = x_1 = -1\n",
    "            inner_lines = []\n",
    "            for item in cluster:\n",
    "                if len(cluster) == 1 and (is_foot(item['text'], foot) or  is_header(item['text'], head)):\n",
    "                    continue\n",
    "                if x_0 == -1:\n",
    "                    x_0 = item['x0']\n",
    "                x_1 = item['x1']\n",
    "                inner_lines.append(item['text'])\n",
    "            if len(inner_lines) > 0:\n",
    "                lines.append(' '.join(inner_lines))\n",
    "                lines_x0_x1.append((x_0, x_1))\n",
    "                \n",
    "            # lines.append(' '.join([i['text'] for i in cluster if not is_foot(i['text'], foot) and not is_header(i['text'], head)]))\n",
    "        elif 'table' in cluster[0]:\n",
    "            lines.append(cluster[0]['table'])\n",
    "            lines_x0_x1.append((10000000, -1))\n",
    "\n",
    "    # Find the minimum of the first elements\n",
    "    if len(lines_x0_x1) == 0:\n",
    "        return []\n",
    "    min_first = min(x[0] for x in lines_x0_x1)\n",
    "\n",
    "    # Find the maximum of the second elements\n",
    "    max_second = max(x[1] for x in lines_x0_x1)\n",
    "       \n",
    "    # print(lines)\n",
    "\n",
    "    for index, item in enumerate(lines):\n",
    "        if type(lines[index]) == str:\n",
    "            lines[index] = lines[index].strip()\n",
    "        # 如果锁进， 那么要换行\n",
    "        \n",
    "        if abs(lines_x0_x1[index][0] - min_first) > 0.1 and lines_x0_x1[index][0] < 1000000:\n",
    "            # print(lines_x0_x1[index][0], min_first, \"----\")\n",
    "            lines[index] = \"\\n\" + lines[index]\n",
    "        # 如果没有写完，那么换行\n",
    "        if abs(lines_x0_x1[index][1] - max_second) > 16 and lines_x0_x1[index][1] > 0:\n",
    "            # print(lines_x0_x1[index][1], max_second, \"----\", lines[index])\n",
    "            lines[index] = lines[index] + \"\\n\"\n",
    "        elif lines_x0_x1[index][1] > 0:\n",
    "            # print('******FULL LINE******', lines_x0_x1[index][1], max_second, \"----\", lines[index])\n",
    "            pass\n",
    "        # print(lines[index], end='')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../corpus/year_report_2.pdf\"\n",
    "pdf = pdfplumber.open(filename)\n",
    "page = pdf.pages[10]\n",
    "page = pdf.pages[15]\n",
    "page = pdf.pages[10]\n",
    "# print(page.extract_text())\n",
    "head, foot = get_foot_header(pdf)\n",
    "print(head)\n",
    "print(foot)\n",
    "get_lines_from_page(page, head, foot)\n",
    "\n",
    "print(pdf.pages[10].extract_words() )# 计算一个字符大概的宽度是多少，然后定义magic distance = 16\n",
    "print(pdf.pages[10].extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfplumber.pdf import PDF\n",
    "import json\n",
    "def begin_mda(lines):\n",
    "    if len(lines) < 1:\n",
    "        return False\n",
    "    if type(lines[0]) != str:\n",
    "        return False\n",
    "    return lines[0].find('管理层讨论与分析') != -1\n",
    "def end_mda(lines):\n",
    "    if len(lines) < 1:\n",
    "        return False\n",
    "    if type(lines[0]) != str:\n",
    "        return False\n",
    "    # print(\"check: \", lines[0], \"$$$$$$\")\n",
    "    return lines[0].find('公司治理') != -1\n",
    "\n",
    "def get_all_lines_about_mda(pdf:PDF, head:str, foot:str):\n",
    "    begin = False\n",
    "    end = False\n",
    "    all_lines = []\n",
    "    for page in pdf.pages:\n",
    "        lines = get_lines_from_page(page, head, foot)\n",
    "        if begin is False and begin_mda(lines):\n",
    "            begin = True\n",
    "        if begin and end_mda(lines):\n",
    "            break\n",
    "        if begin is True:\n",
    "            all_lines.append(lines)\n",
    "\n",
    "    content_list = []\n",
    "    for lines in all_lines:\n",
    "        for line in lines:\n",
    "            if type(line) != str:\n",
    "                line = json.dumps(line, ensure_ascii=False)\n",
    "            # print(line, end='')\n",
    "            content_list.append(line)\n",
    "    return \"\".join(content_list)\n",
    "            \n",
    "content = get_all_lines_about_mda(pdf, head, foot)\n",
    "print(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抓取信息\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def split_interval(start, end, day_cnt):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    start_date = datetime.strptime(start, date_format)\n",
    "    end_date = datetime.strptime(end, date_format)\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    while start_date < end_date:\n",
    "        # Calculate the end date of the current interval.\n",
    "        interval_end_date = start_date + timedelta(days=day_cnt-1)\n",
    "\n",
    "        # Ensure that the interval doesn't go beyond the overall end date.\n",
    "        if interval_end_date > end_date:\n",
    "            interval_end_date = end_date\n",
    "\n",
    "        # Append the current interval to the result list.\n",
    "        result.append(\n",
    "            (start_date.strftime(date_format), \n",
    "             interval_end_date.strftime(date_format), \n",
    "             (interval_end_date - start_date).days + 1)\n",
    "        )\n",
    "\n",
    "        # Move the start date to the next day after the current interval.\n",
    "        start_date = interval_end_date + timedelta(days=1)\n",
    "\n",
    "    return result\n",
    "results = split_interval('2023-01-01', '2023-07-31', 1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def request_data(page_size=1000, page_num=1, se_date=\"2023-04-26~2023-04-26\"):\n",
    "    url = \"http://www.cninfo.com.cn/new/hisAnnouncement/query\"\n",
    "    if page_num > 1:\n",
    "        url = \"http://www.cninfo.com.cn/new/hisAnnouncement/query\"\n",
    "\n",
    "    params = {\n",
    "        \"pageNum\": page_num,\n",
    "        \"pageSize\": page_size,\n",
    "        \"column\": \"szse\",\n",
    "        \"tabName\": \"fulltext\",\n",
    "        \"plate\": \"\",\n",
    "        \"stock\": \"\",\n",
    "        \"searchkey\": \"\",\n",
    "        \"secid\": \"\",\n",
    "        \"category\": \"category_ndbg_szsh\",\n",
    "        \"trade\": \"\",\n",
    "        \"seDate\": se_date,\n",
    "        \"sortName\": \"\",\n",
    "        \"sortType\": \"\",\n",
    "        \"isHLtitle\": \"true\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, data=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # 或者返回response.text取决于你需要什么样的数据格式\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "file = open('report_meta_info.txt', 'w')\n",
    "results = split_interval('2023-01-01', '2023-07-31', 1)\n",
    "results = results[::-1]\n",
    "#for date_range in results:\n",
    "for i, date_range in enumerate(tqdm(results)):\n",
    "    se_date = f\"{date_range[0]}~{date_range[1]}\"\n",
    "    \n",
    "    page_size = 30\n",
    "    data = request_data(page_size=10, se_date=se_date)\n",
    "    try_times = 0\n",
    "    while data is None:\n",
    "        time.sleep(1)\n",
    "        data = request_data(page_size=10, se_date=se_date)\n",
    "        try_times += 1\n",
    "        print(f\"craw data wrong, try {try_times}\")\n",
    "        if try_times == 3:\n",
    "            break  \n",
    "    if data is None:\n",
    "        continue\n",
    "    page_count = (data['totalAnnouncement'] -1 )// page_size  + 1\n",
    "    print(page_count)\n",
    "    begin = time.time()\n",
    "    for index in range(1, page_count+1):\n",
    "    \n",
    "        data = request_data(page_num=index, page_size=page_size, se_date=se_date)\n",
    "        try_times = 0\n",
    "        while data is None:\n",
    "        \n",
    "            time.sleep(3)\n",
    "            data = request_data(page_num=index, page_size=page_size, se_date=se_date)\n",
    "            try_times += 1\n",
    "            print(f\"craw data wrong, try {try_times}\")\n",
    "            if try_times == 3:\n",
    "                break\n",
    "        \n",
    "        last = time.time() - begin\n",
    "        need_time = last/index * page_count  - last\n",
    "        every_time = last*1./index\n",
    "        print(f\"{index}/{page_count}: last time {last} 秒， need {need_time}, every page cost: {every_time}, count: { len(data['announcements'])}, but page size is {page_size}\")\n",
    "        for item in data['announcements']:\n",
    "            file.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "file.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
