{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 管理层信息抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /Users/alchemy_taotaox/Desktop/next/my_test/* ../corpus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法基本思路\n",
    "1. 扫描pdf 获得页眉页脚\n",
    "2. 扫描pdf 获得最小和最大的x坐标\n",
    "    如果一个字符是最小的坐标，那么就和之前的行合并为一个段落\n",
    "    如果一个字符是最大的坐标，那么就和之前的\n",
    "2. 提取文档中的表格和字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfplumber.pdf import PDF\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def is_header(str, header):\n",
    "    if header is None:\n",
    "        return False\n",
    "    return str == header\n",
    "\n",
    "def is_foot(str, foot):\n",
    "    if foot is None:\n",
    "        return False\n",
    "    \n",
    "    s = re.sub(r'\\s*\\d+\\s*', '#num#', str)\n",
    "    return s == foot or s == '#num#/#num#'\n",
    "\n",
    "def get_foot_header(pdf:PDF, check_pages_num=20, threshold = 0.9):\n",
    "    head_counter = Counter()\n",
    "    foot_counter = Counter()\n",
    "    pages_num = len(pdf.pages)\n",
    "    if pages_num < check_pages_num:\n",
    "        check_pages_num = pages_num\n",
    "    for i in range(check_pages_num):\n",
    "        text_lines = pdf.pages[i].extract_words()\n",
    "        if text_lines is not None and len(text_lines) == 0:\n",
    "            continue\n",
    "        top_line = text_lines[0]['text']\n",
    "        bottom_line = text_lines[-1]['text']\n",
    "        # print(bottom_line, \" !!!!! \")\n",
    "        bottom_line = re.sub(r'\\d+', '#num#', bottom_line)\n",
    "        head_counter.update([top_line])\n",
    "        foot_counter.update([bottom_line])\n",
    "    head, foot = \"fadfasdfa\", \"dfadfadsfsdf\"\n",
    "    try:    \n",
    "        most_common_head_word, count_head = head_counter.most_common(1)[0]\n",
    "        most_common_foot_word, count_foot = foot_counter.most_common(1)[0]\n",
    "\n",
    "   \n",
    "        if count_head *1./check_pages_num >= threshold:\n",
    "            head = most_common_head_word\n",
    "        if count_foot *1./check_pages_num >= threshold:\n",
    "            foot = most_common_foot_word\n",
    "    except:\n",
    "        pass\n",
    "    return head, foot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import json\n",
    "\n",
    "def check_bboxes(word, table_bbox):\n",
    "    \"\"\"\n",
    "    Check whether word is inside a table bbox.\n",
    "    \"\"\"\n",
    "    l = word['x0'], word['top'], word['x1'], word['bottom']\n",
    "    r = table_bbox\n",
    "    return l[0] > r[0] and l[1] > r[1] and l[2] < r[2] and l[3] < r[3]\n",
    "\n",
    "\n",
    "\n",
    "def get_lines_from_page(page, head, foot):\n",
    "    tables = page.find_tables()\n",
    "    table_bboxes = [i.bbox for i in tables]\n",
    "    tables = [{'table': i.extract(), 'top': i.bbox[1]} for i in tables]\n",
    "    non_table_words = [word for word in page.extract_words() if not any(\n",
    "        [check_bboxes(word, table_bbox) for table_bbox in table_bboxes])]\n",
    "    lines = []\n",
    "    lines_x0_x1 =[]\n",
    "    for cluster in pdfplumber.utils.cluster_objects(\n",
    "        non_table_words + tables, itemgetter('top'), tolerance=5):\n",
    "        if 'text' in cluster[0]:\n",
    "            x_0 = x_1 = -1\n",
    "            inner_lines = []\n",
    "            for item in cluster:\n",
    "                #if len(cluster) == 1 and (is_foot(item['text'], foot) or  is_header(item['text'], head)):\n",
    "                #    continue\n",
    "                # print(item, cluster)\n",
    "                if x_0 == -1:\n",
    "                    if 'x0' in item:\n",
    "                        x_0 = item['x0']\n",
    "                if 'x1' in item:\n",
    "                    x_1 = item['x1']\n",
    "                if 'text' in item:\n",
    "                    inner_lines.append(item['text'])\n",
    "                elif 'table' in item:\n",
    "                    inner_lines.append(\"\\n\" + json.dumps(item['table'], ensure_ascii=False) + \"\\n\")\n",
    "            if len(inner_lines) > 0:\n",
    "                lines.append(' '.join(inner_lines))\n",
    "                lines_x0_x1.append((x_0, x_1))\n",
    "                \n",
    "            # lines.append(' '.join([i['text'] for i in cluster if not is_foot(i['text'], foot) and not is_header(i['text'], head)]))\n",
    "        elif 'table' in cluster[0]:\n",
    "            lines.append(cluster[0]['table'])\n",
    "            lines_x0_x1.append((10000000, -1))\n",
    "\n",
    "    # Find the minimum of the first elements\n",
    "    if len(lines_x0_x1) == 0:\n",
    "        return []\n",
    "    min_first = min(x[0] for x in lines_x0_x1)\n",
    "\n",
    "    # Find the maximum of the second elements\n",
    "    max_second = max(x[1] for x in lines_x0_x1)\n",
    "       \n",
    "    # print(lines)\n",
    "    \n",
    "    # lines = lines[1:-1]\n",
    "    if type(lines[0]) == str and is_header(lines[0], head):\n",
    "        lines = lines[1:]\n",
    "    if type(lines[-1]) == str and is_foot(lines[-1], foot):\n",
    "        lines = lines[:-1]\n",
    "    for index, item in enumerate(lines):\n",
    "        if type(lines[index]) == str:\n",
    "            lines[index] = lines[index].strip()\n",
    "        \n",
    "        # 如果锁进， 那么要换行\n",
    "        \n",
    "        if abs(lines_x0_x1[index][0] - min_first) > 0.1 and lines_x0_x1[index][0] < 1000000:\n",
    "            # print(lines_x0_x1[index][0], min_first, \"----\")\n",
    "            if type(lines[index]) == str:\n",
    "                lines[index] = \"\\n\" + lines[index]\n",
    "        # 如果没有写完，那么换行\n",
    "        if abs(lines_x0_x1[index][1] - max_second) > 16 and lines_x0_x1[index][1] > 0:\n",
    "            # print(lines_x0_x1[index][1], max_second, \"----\", lines[index])\n",
    "            if type(lines[index]) == str:\n",
    "                lines[index] = lines[index] + \"\\n\"\n",
    "        elif lines_x0_x1[index][1] > 0:\n",
    "            # print('******FULL LINE******', lines_x0_x1[index][1], max_second, \"----\", lines[index])\n",
    "            pass\n",
    "        # print(lines[index], end='')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../corpus/year_report_2.pdf\"\n",
    "filename = \"/tmp/b.PDF\"\n",
    "# filename = \"/tmp/c.PDF\"\n",
    "# filename = \"/tmp/e.PDF\"\n",
    "filename = \"/tmp/f.PDF\"\n",
    "pdf = pdfplumber.open(filename)\n",
    "# page = pdf.pages[10]\n",
    "# page = pdf.pages[15]\n",
    "page = pdf.pages[7]\n",
    "# print(page.extract_text())\n",
    "head, foot = get_foot_header(pdf)\n",
    "print(head)\n",
    "print(foot)\n",
    "lines = get_lines_from_page(page, head, foot)\n",
    "for line in lines:\n",
    "    print(\"----\", line, \"----\", type(line))\n",
    "\n",
    "# print(pdf.pages[7].extract_words() )# 计算一个字符大概的宽度是多少，然后定义magic distance = 16\n",
    "# print(pdf.pages[7].extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfplumber.pdf import PDF\n",
    "import json\n",
    "def begin_mda(lines):\n",
    "    len_lines = len(lines)\n",
    "    if len_lines < 1:\n",
    "        return False, -1\n",
    "    # if type(lines[0]) != str:\n",
    "    #     return False, -1\n",
    "    # print(\" check beign count \", len_lines)\n",
    "    for index, item in enumerate(lines):\n",
    "        # print(\"CHECK\", item, index)\n",
    "        if type(item) != str:\n",
    "            continue\n",
    "        if item.find('管理层讨论与分析') != -1 and item.find('第三节')!= -1 and item.find(\"...............\") == -1 and item.find(\"请\") == -1:\n",
    "            # print(' BEGIN ', index)\n",
    "            return True, index\n",
    "    return False, -1\n",
    "def end_mda(lines):\n",
    "    if len(lines) < 1:\n",
    "        return False, -1\n",
    "    # if type(lines[0]) != str:\n",
    "    #     return False, -1\n",
    "    # print(\"check: \", lines[0], \"$$$$$$\")\n",
    "    for index, item in enumerate(lines):\n",
    "        if type(item) != str:\n",
    "            continue\n",
    "        if item.find('公司治理') != -1 and item.find('第四节')!= -1 and item.find(\"...............\") == -1 and item.find(\"请\") == -1:\n",
    "            return True, index\n",
    "    return False, -1\n",
    "\n",
    "def get_all_lines_about_mda(pdf:PDF, head:str, foot:str):\n",
    "    begin = False\n",
    "    end = False\n",
    "    all_lines = []\n",
    "    index = 0\n",
    "    # print(f\"page number: {len(pdf.pages)}\")\n",
    "    for page in pdf.pages:\n",
    "        index += 1\n",
    "        if index < 5:\n",
    "            continue\n",
    "        # print(index, \" ----- page \", head, foot)\n",
    "        lines = get_lines_from_page(page, head, foot)\n",
    "        # for line in lines:\n",
    "        #     print(line)\n",
    "        begin_mda_status, line_index = begin_mda(lines)\n",
    "        # print(index, ' ---page   check ------', begin_mda_status)\n",
    "\n",
    "        if begin is False and begin_mda_status:\n",
    "            begin = True\n",
    "            lines = lines[line_index:]\n",
    "        end_mda_status, line_index = end_mda(lines)\n",
    "        if begin and end_mda_status:\n",
    "            \n",
    "            all_lines.append(lines[:line_index])\n",
    "            break\n",
    "        if begin is True:\n",
    "            all_lines.append(lines)\n",
    "    # print(f\" final page index {index}\")\n",
    "    content_list = []\n",
    "    for lines in all_lines:\n",
    "        for line in lines:\n",
    "            if type(line) != str:\n",
    "                line = json.dumps(line, ensure_ascii=False)\n",
    "            # print(line, end='')\n",
    "            content_list.append(line)\n",
    "    return \"\".join(content_list)\n",
    "            \n",
    "content = get_all_lines_about_mda(pdf, head, foot)\n",
    "if content is not None:\n",
    "    print(content)\n",
    "else:\n",
    "    print(\"None\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抓取信息\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def split_interval(start, end, day_cnt):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    start_date = datetime.strptime(start, date_format)\n",
    "    end_date = datetime.strptime(end, date_format)\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    while start_date < end_date:\n",
    "        # Calculate the end date of the current interval.\n",
    "        interval_end_date = start_date + timedelta(days=day_cnt-1)\n",
    "\n",
    "        # Ensure that the interval doesn't go beyond the overall end date.\n",
    "        if interval_end_date > end_date:\n",
    "            interval_end_date = end_date\n",
    "\n",
    "        # Append the current interval to the result list.\n",
    "        result.append(\n",
    "            (start_date.strftime(date_format), \n",
    "             interval_end_date.strftime(date_format), \n",
    "             (interval_end_date - start_date).days + 1)\n",
    "        )\n",
    "\n",
    "        # Move the start date to the next day after the current interval.\n",
    "        start_date = interval_end_date + timedelta(days=1)\n",
    "\n",
    "    return result\n",
    "results = split_interval('2023-01-01', '2023-07-31', 1)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def request_data(page_size=1000, page_num=1, se_date=\"2023-04-26~2023-04-26\"):\n",
    "    url = \"http://www.cninfo.com.cn/new/hisAnnouncement/query\"\n",
    "    if page_num > 1:\n",
    "        url = \"http://www.cninfo.com.cn/new/hisAnnouncement/query\"\n",
    "\n",
    "    params = {\n",
    "        \"pageNum\": page_num,\n",
    "        \"pageSize\": page_size,\n",
    "        \"column\": \"szse\",\n",
    "        \"tabName\": \"fulltext\",\n",
    "        \"plate\": \"\",\n",
    "        \"stock\": \"\",\n",
    "        \"searchkey\": \"\",\n",
    "        \"secid\": \"\",\n",
    "        \"category\": \"category_ndbg_szsh\",\n",
    "        \"trade\": \"\",\n",
    "        \"seDate\": se_date,\n",
    "        \"sortName\": \"\",\n",
    "        \"sortType\": \"\",\n",
    "        \"isHLtitle\": \"true\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, data=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # 或者返回response.text取决于你需要什么样的数据格式\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "file = open('report_meta_info.txt', 'w')\n",
    "results = split_interval('2023-01-01', '2023-07-31', 1)\n",
    "results = results[::-1]\n",
    "#for date_range in results:\n",
    "for i, date_range in enumerate(tqdm(results)):\n",
    "    se_date = f\"{date_range[0]}~{date_range[1]}\"\n",
    "    \n",
    "    page_size = 30\n",
    "    data = request_data(page_size=10, se_date=se_date)\n",
    "    try_times = 0\n",
    "    while data is None:\n",
    "        time.sleep(1)\n",
    "        data = request_data(page_size=10, se_date=se_date)\n",
    "        try_times += 1\n",
    "        print(f\"craw data wrong, try {try_times}\")\n",
    "        if try_times == 3:\n",
    "            break  \n",
    "    if data is None:\n",
    "        continue\n",
    "    page_count = (data['totalAnnouncement'] -1 )// page_size  + 1\n",
    "    print(page_count)\n",
    "    begin = time.time()\n",
    "    for index in range(1, page_count+1):\n",
    "    \n",
    "        data = request_data(page_num=index, page_size=page_size, se_date=se_date)\n",
    "        try_times = 0\n",
    "        while data is None:\n",
    "        \n",
    "            time.sleep(3)\n",
    "            data = request_data(page_num=index, page_size=page_size, se_date=se_date)\n",
    "            try_times += 1\n",
    "            print(f\"craw data wrong, try {try_times}\")\n",
    "            if try_times == 3:\n",
    "                break\n",
    "        \n",
    "        last = time.time() - begin\n",
    "        need_time = last/index * page_count  - last\n",
    "        every_time = last*1./index\n",
    "        print(f\"{index}/{page_count}: last time {last} 秒， need {need_time}, every page cost: {every_time}, count: { len(data['announcements'])}, but page size is {page_size}\")\n",
    "        for item in data['announcements']:\n",
    "            file.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def download_and_process_file(url, local_path='/tmp/a.PDF', try_count=3):\n",
    "    try_times = 0\n",
    "    response = None\n",
    "    while True:\n",
    "        # 下载文件\n",
    "        response = requests.get(url, proxies={})\n",
    "        if response.status_code == 200:\n",
    "            break\n",
    "            # return response.json()  # 或者返回response.text取决于你需要什么样的数据格式\n",
    "        else:\n",
    "            response = None\n",
    "        try_times += 1\n",
    "        if try_times >= try_count:\n",
    "            time.sleep(3)\n",
    "            break\n",
    "    if response is None:\n",
    "        return response\n",
    "    # response.raise_for_status()  # 确保请求成功\n",
    "\n",
    "    # 将文件写入本地\n",
    "    with open(local_path, 'wb') as output_file:\n",
    "        output_file.write(response.content)\n",
    "\n",
    "    # 读取PDF文件的内容\n",
    "    \n",
    "    pdf = pdfplumber.open(local_path)\n",
    "    head, foot = get_foot_header(pdf)\n",
    "    content = get_all_lines_about_mda(pdf, head, foot)\n",
    "    #print(content)\n",
    "    # 删除临时文件\n",
    "    os.remove(local_path)\n",
    "\n",
    "    return content\n",
    "'''\n",
    "1. read meta file\n",
    "2. download pdf file into tmp file\n",
    "3. get the necessary content\n",
    "4. write data into file\n",
    "   {'stock_code', 'stock_name', 'content':   'url':  'pubtime': xxx}\n",
    "\n",
    "'''\n",
    "import os\n",
    "if 'all_proxy' in os.environ:\n",
    "    print(\"pop all proxy\")\n",
    "    os.environ.pop('all_proxy')\n",
    "base_url = 'http://static.cninfo.com.cn/'\n",
    "meta_file = 'report_meta_info.txt'\n",
    "data_file = 'report_data_info.txt'\n",
    "file = open(data_file, \"w\")\n",
    "lines = open(meta_file).readlines()\n",
    "code_set = set()\n",
    "for index, line in tqdm(enumerate(lines)):\n",
    "    # if index < 60:\n",
    "    #     continue\n",
    "    info = json.loads(line.strip())\n",
    "    '''\n",
    "    \"secCode\": \"001211\", \"secName\": \"双枪科技, adjunctUrl\n",
    "    '''\n",
    "    title = info['announcementTitle']\n",
    "    if title.find('关于') != -1 or title.find(\"摘要\") != -1 or title.find('2022') == -1 or title.find(\"英文\") != -1:\n",
    "        continue\n",
    "    stock_code = info['secCode']\n",
    "    if stock_code in code_set:\n",
    "        continue\n",
    "    # code_set.add(stock_code)\n",
    "    stock_name = info['secName']\n",
    "    relative_url = info['adjunctUrl']\n",
    "    pub_time = relative_url.split('/')[-2]\n",
    "    url =  urljoin(base_url, relative_url)\n",
    "    content = download_and_process_file(url, \"/tmp/a.PDF\")\n",
    "    if content is None or content == \"\":\n",
    "        pass\n",
    "    else:\n",
    "        code_set.add(stock_code)\n",
    "    result = {\"title\": title, 'pubtime': pub_time, 'stock_code': stock_code, 'stock_name': stock_name, 'content': content, 'url': url}\n",
    "    file.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "    # if index == 100:\n",
    "    #     break\n",
    "\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5126  ----- \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "process_item() missing 1 required positional argument: 'lock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/mnt/workspace/taozw/local/soft/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/mnt/workspace/taozw/local/soft/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\nTypeError: process_item() missing 1 required positional argument: 'lock'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m     lock \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39mLock()\n\u001b[1;32m     65\u001b[0m     \u001b[39m# processed_set = manager.set()\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m     \u001b[39m# 使用进程池并行处理数组中的每个元素\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     pool\u001b[39m.\u001b[39;49mmap(process_item, [(item, lock) \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m new_data_list[:\u001b[39m100\u001b[39;49m]])\n\u001b[1;32m     70\u001b[0m \u001b[39m# 关闭进程池\u001b[39;00m\n\u001b[1;32m     71\u001b[0m pool\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/mnt/workspace/taozw/local/soft/anaconda3/lib/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/mnt/workspace/taozw/local/soft/anaconda3/lib/python3.9/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mTypeError\u001b[0m: process_item() missing 1 required positional argument: 'lock'"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def process_item(args):\n",
    "    \n",
    "    item, lock = args\n",
    "    info = json.loads(item.strip())\n",
    "    '''\n",
    "    \"secCode\": \"001211\", \"secName\": \"双枪科技, adjunctUrl\n",
    "    '''\n",
    "    title = info['announcementTitle']\n",
    "    if title.find('关于') != -1 or title.find(\"摘要\") != -1 or title.find('2022') == -1 or title.find(\"英文\") != -1:\n",
    "        return\n",
    "    stock_code = info['secCode']\n",
    "    # with lock:\n",
    "        # if stock_code in code_set:\n",
    "            # return\n",
    "    # code_set.add(stock_code)\n",
    "    stock_name = info['secName']\n",
    "    relative_url = info['adjunctUrl']\n",
    "    pub_time = relative_url.split('/')[-2]\n",
    "    url =  urljoin(base_url, relative_url)\n",
    "    content = download_and_process_file(url, \"/tmp/a.PDF\")\n",
    "    if content is None or content == \"\":\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "        # with lock:\n",
    "            # code_set.add(stock_code)\n",
    "    result = {\"title\": title, 'pubtime': pub_time, 'stock_code': stock_code, 'stock_name': stock_name, 'content': content, 'url': url}\n",
    "    with lock:\n",
    "        with open(\"output.txt\", \"a\") as file:\n",
    "            # file.write(str(result) + \"\\n\")\n",
    "            file.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# 定义要处理的数组\n",
    "data_list = lines  # 请替换为实际的数组元素\n",
    "new_data_list = []\n",
    "stock_code_set = set()\n",
    "for item in data_list:\n",
    "    info = json.loads(item.strip())\n",
    "    '''\n",
    "    \"secCode\": \"001211\", \"secName\": \"双枪科技, adjunctUrl\n",
    "    '''\n",
    "    title = info['announcementTitle']\n",
    "    if title.find('关于') != -1 or title.find(\"摘要\") != -1 or title.find('2022') == -1 or title.find(\"英文\") != -1:\n",
    "        continue\n",
    "    stock_code = info['secCode']\n",
    "    if stock_code in stock_code_set:\n",
    "        continue\n",
    "    stock_code_set.add(stock_code)\n",
    "    new_data_list.append(item)\n",
    "\n",
    "    \n",
    "print(len(new_data_list), \" ----- \")\n",
    "# 定义要开启的进程数量\n",
    "n = 6  # 请替换为实际需要的进程数量\n",
    "\n",
    "# 创建进程池\n",
    "pool = multiprocessing.Pool(processes=n)\n",
    "\n",
    "# 创建进程锁和共享的集合\n",
    "with multiprocessing.Manager() as manager:\n",
    "    lock = manager.Lock()\n",
    "    # processed_set = manager.set()\n",
    "\n",
    "    # 使用进程池并行处理数组中的每个元素\n",
    "    pool.map(process_item, [(item, lock) for item in new_data_list[:100]])\n",
    "\n",
    "# 关闭进程池\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
